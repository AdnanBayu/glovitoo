{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMEQbCgWaVLyYzkx+70yCO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamtemtomm/glovitoo/blob/main/glovitoo_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <p> Import data\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!unzip data.zip\n",
        "!rm data.zip >& /dev/null"
      ],
      "metadata": {
        "id": "cQA16cBEiLh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Essential Import"
      ],
      "metadata": {
        "id": "YfqwiVN_YiN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <p>Essential Import\n",
        "import os, shutil, json\n",
        "from PIL import Image\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np, pandas as pd, random as rd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z6udXc74U1us"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <p>Torch Essential Import\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "torch.manual_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0x6axOJraP5S"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Import"
      ],
      "metadata": {
        "id": "EYX8HLfYYlJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <p> Get data from the file\n",
        "DATA_DIR  = 'data'\n",
        "TRAIN_DATA_DICT = {}\n",
        "VAL_DATA_DICT = {}\n",
        "VAL_SIZE= 0.2\n",
        "THRESHOLD = 20\n",
        "# len_array = []\n",
        "\n",
        "for letter in os.listdir(DATA_DIR):\n",
        "  data_size = VAL_SIZE * len(os.listdir(os.path.join(DATA_DIR, letter)))\n",
        "  for i, file_data in enumerate(os.listdir(os.path.join(DATA_DIR, letter))):\n",
        "    with open(os.path.join(DATA_DIR, letter, file_data), 'r') as f:\n",
        "      try :\n",
        "        data = f.read().splitlines()\n",
        "        # Check if the data is reach minimum threshold\n",
        "        if len(data) < THRESHOLD:\n",
        "          continue\n",
        "\n",
        "        data_array = []\n",
        "        for data_line in data[-THRESHOLD:]:\n",
        "          data_array.append([float(a) for a in data_line.split(', ')])\n",
        "        # len_array.append(len(data_array))\n",
        "        if i < data_size :\n",
        "          VAL_DATA_DICT[file_data.split('.')[0]] =  np.array(data_array)\n",
        "        else :\n",
        "          TRAIN_DATA_DICT[file_data.split('.')[0]] =  np.array(data_array)\n",
        "\n",
        "      except :\n",
        "        continue"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KZedjJYlVWJt"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <p> Initialized SIBIDataset\n",
        "class SIBIDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    self.data_dict = data_dict\n",
        "    self.features = []\n",
        "    self.labels = []\n",
        "\n",
        "    for label, features in data_dict.items():\n",
        "      self.features.append(torch.from_numpy(features).float())\n",
        "      self.labels.append(F.one_hot(torch.tensor(ord(label.split('-')[0]) - 97), num_classes=26))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "uPFx59lQawsF",
        "cellView": "form"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <p> Trainset and Trainloader\n",
        "trainset = SIBIDataset(TRAIN_DATA_DICT)\n",
        "valset = SIBIDataset(VAL_DATA_DICT)\n",
        "\n",
        "trainloader = DataLoader(trainset,batch_size=4,shuffle=True,num_workers=1)\n",
        "valloader = DataLoader(trainset,batch_size=4,shuffle=True,num_workers=1)"
      ],
      "metadata": {
        "id": "PCMOZb36rhTE"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialized model"
      ],
      "metadata": {
        "id": "jyFEDoxREBjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialized RNN Model\n",
        "A\n"
      ],
      "metadata": {
        "id": "xGLn2xjNiYFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <p>Make training loop\n",
        "\n",
        "def train_loop(train_loader, val_loader, epoch_num, patience, model, loss_fn, optimizer, device, target_acc = 1):\n",
        "\n",
        "  best_metric, best_metric_epoch, cur_patience = -1, -1, 0\n",
        "  epoch_loss_values, metric_values = list(), list()\n",
        "  prev_acc_metric = 0\n",
        "\n",
        "  for epoch in range(epoch_num):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
        "    epoch_loss, step = 0, 1\n",
        "    steps_per_epoch = data_len // train_loader.batch_size\n",
        "\n",
        "    model.train()\n",
        "    for batch_data in train_loader:\n",
        "\n",
        "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if (step) % 10 == 0 :\n",
        "          print(f\"{step}/{data_len // train_loader.batch_size + 1}, training_loss: {loss.item():.4f}\")\n",
        "        step += 1\n",
        "\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    if (epoch + 1) % 10 == 0 :\n",
        "      print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds, labels = list(), list()\n",
        "        for val_data in val_loader:\n",
        "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
        "\n",
        "            val_pred = model(val_images)\n",
        "\n",
        "            preds.append(val_pred)\n",
        "            labels.append(val_labels)\n",
        "\n",
        "        y_pred = torch.cat(preds)\n",
        "        y = torch.cat(labels)\n",
        "\n",
        "        true_pred = [torch.argmax(x1) == torch.argmax(x2) for x1, x2 in zip(y_pred, y)]\n",
        "        acc_metric = float(sum(1 for i in true_pred if i))/len(y_pred)\n",
        "        print(f'Accuracy : {acc_metric}')\n",
        "        metric_values.append(acc_metric)\n",
        "\n",
        "        if acc_metric > best_metric:\n",
        "            best_metric = acc_metric\n",
        "            best_metric_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), os.path.join(model_path, \"best.pth\"))\n",
        "            shutil.copy(os.path.join(model_path, \"best.pth\"), os.path.join(\"/content/gdrive/MyDrive/compfestTrain/\",str(date.today()),\"best.pth\"))\n",
        "            print(\"saved new best metric network\")\n",
        "        if acc_metric <= prev_acc_metric : cur_patience += 1\n",
        "        else : cur_patience = 0\n",
        "\n",
        "        prev_acc_metric = acc_metric\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(model_path, \"last.pth\"))\n",
        "        shutil.copy(os.path.join(model_path, \"last.pth\"), os.path.join(\"/content/gdrive/MyDrive/compfestTrain/\",str(date.today()),\"last.pth\"))\n",
        "\n",
        "        print(\n",
        "            f\"current epoch: {epoch + 1} / \"\n",
        "            f\"current accuracy: {acc_metric:.4f} best ACC: {best_metric:.4f} \"\n",
        "            f\"at epoch: {best_metric_epoch}/ \"\n",
        "            f\"cur patience: {cur_patience}\"\n",
        "            )\n",
        "\n",
        "        if acc_metric == target_acc:\n",
        "          print(\"Got target accuracy, Stop the training session\")\n",
        "          break\n",
        "\n",
        "        if cur_patience == patience:\n",
        "          print(f\"Callback Activated, Stop the training session\")\n",
        "          break\n",
        "\n",
        "  return epoch_loss_values, metric_values"
      ],
      "metadata": {
        "id": "hZ3SancMDI-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}